# Rapport Complet: Probl√®mes & Solutions - G√©n√©ration d'Articles GEO Optimis√©s

**Date**: 2025-11-20
**Contexte**: Impl√©mentation d'un syst√®me de g√©n√©ration d'articles SEO/GEO avec workflow asynchrone utilisant GPT-5.1

---

## Table des Mati√®res

1. [Architecture G√©n√©rale](#architecture-g√©n√©rale)
2. [Chronologie des Probl√®mes](#chronologie-des-probl√®mes)
3. [Probl√®me Actuel (BLOQUANT)](#probl√®me-actuel-bloquant)
4. [Solutions Propos√©es](#solutions-propos√©es)
5. [Fichiers Modifi√©s](#fichiers-modifi√©s)
6. [Tests Effectu√©s](#tests-effectu√©s)

---

## Architecture G√©n√©rale

### Workflow Complet

```
workflow_start
    ‚Üì
research (Perplexity Sonar)
    ‚Üì
draft_sections (NEW - G√©n√©ration par sections)
    ‚îú‚îÄ Section 0: H1 + Intro (2500 tokens)
    ‚îú‚îÄ Section 1: H2 (2500 tokens)
    ‚îú‚îÄ Section 2: H2 (2500 tokens)
    ‚îú‚îÄ Section 3: H2 (2500 tokens)
    ‚îú‚îÄ Section 4: H2 (2500 tokens)
    ‚îî‚îÄ Section 5: FAQ + Conclusion (2500 tokens)
    ‚Üì
assemble_article (Reconstruction depuis DB)
    ‚Üì
review (Scoring)
    ‚Üì
rewrite (si score < minScore)
    ‚Üì
completed
```

### Stack Technique

- **Backend**: Vercel Edge Functions (api/geo.ts)
- **Database**: Supabase (PostgreSQL + Storage)
- **LLM**: OpenAI GPT-5.1 (gpt-5.1-turbo-2025-07-15)
- **Research**: Perplexity Sonar
- **Deployment**: Vercel (auto-deploy via git push)

---

## Chronologie des Probl√®mes

### PROBL√àME 1: Troncation HTTP des Articles Volumineux

**Description**:
Les articles g√©n√©r√©s (~3500+ mots, 80-150 KB HTML) √©taient tronqu√©s lors du retour HTTP. Les clients ne recevaient que les premiers ~30-50 KB de contenu.

**Cause Racine**:
- Limite de payload HTTP sur Vercel Edge Functions
- G√©n√©ration monolithique en 2 parties (8000 tokens √ó 2) cr√©ait des r√©ponses trop volumineuses
- Le fichier JSON complet de l'article d√©passait les limites de buffer HTTP

**Sympt√¥mes Observ√©s**:
```javascript
// L'API retournait ceci:
{
  "article": "<h1>Titre</h1><p>Contenu...</p><h2>Section 1</h2><p>..." // TRONQU√â ICI
}
// Manque: Sections 2, 3, 4, FAQ, Conclusion
```

**Impact**:
- Articles incomplets livr√©s aux utilisateurs
- Perte de ~50% du contenu
- Impossibilit√© d'atteindre le minimum requis de 2000 mots

**SOLUTION IMPL√âMENT√âE**: G√©n√©ration Sectionnelle + Stockage DB

**Changements architecturaux majeurs**:

1. **Remplacement de `draft_part1` et `draft_part2` par `draft_sections`**:
   - Ancien: 2 appels GPT-5.1 √ó 8000 tokens = articles trop volumineux
   - Nouveau: 6 appels GPT-5.1 √ó 2500 tokens = sections individuelles

2. **Cr√©ation de la table `articles_content` (Supabase)**:
```sql
CREATE TABLE articles_content (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  job_id TEXT NOT NULL,
  section_index INTEGER NOT NULL,
  section_id TEXT,
  section_title TEXT,
  content JSONB NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(job_id, section_index)
);

CREATE INDEX idx_articles_content_job_id ON articles_content(job_id);
CREATE INDEX idx_articles_content_job_section ON articles_content(job_id, section_index);
```

3. **Ajout de fonctions helpers dans api/geo.ts** (lignes 45-89):
```typescript
async function saveSection(jobId: string, index: number, id: string, title: string, data: any) {
  const { data: existing } = await supabase
    .from('articles_content')
    .select('id')
    .eq('job_id', jobId)
    .eq('section_index', index)
    .single();

  if (existing) {
    await supabase.from('articles_content').update({
      section_id: id,
      section_title: title,
      content: data,
      updated_at: new Date().toISOString()
    }).eq('job_id', jobId).eq('section_index', index);
  } else {
    await supabase.from('articles_content').insert({
      job_id: jobId,
      section_index: index,
      section_id: id,
      section_title: title,
      content: data,
      created_at: new Date().toISOString(),
      updated_at: new Date().toISOString()
    });
  }
}

async function getAllSections(jobId: string) {
  const { data } = await supabase
    .from('articles_content')
    .select('*')
    .eq('job_id', jobId)
    .order('section_index', { ascending: true });
  return data || [];
}
```

4. **Nouvelle √©tape `assemble_article`** (lignes 1199-1262):
   - R√©cup√®re toutes les sections depuis la DB
   - Reconstruit l'article complet
   - Sauvegarde uniquement l'HTML final (pas de retour HTTP volumineux)

**R√©sultat Attendu**:
- ‚úÖ Chaque section g√©n√©r√©e individuellement (< 10 KB)
- ‚úÖ Stockage en base de donn√©es (pas de limite)
- ‚úÖ Reconstruction en arri√®re-plan
- ‚úÖ Articles complets livr√©s

**Commit D√©ploy√©**: `b59a272`

---

### PROBL√àME 2: Table `articles_content` Inexistante

**Description**:
Apr√®s d√©ploiement du code `draft_sections`, le workflow plantait avec:
```
Failed to save section 0: Could not find the table 'public.articles_content' in the schema cache
```

**Cause Racine**:
- La table n'existait pas encore dans Supabase
- Le code tentait d'ins√©rer des sections dans une table non cr√©√©e

**Tentative de Solution Automatique (√âCHEC)**:
Cr√©ation du script `create-table-articles-content.js` pour cr√©er la table via l'API Supabase:
```javascript
const response = await fetch(`${SUPABASE_URL}/rest/v1/rpc/exec_sql`, {
  method: 'POST',
  headers: { 'apikey': SUPABASE_KEY },
  body: JSON.stringify({ query: SQL })
});
```

**Erreur Rencontr√©e**:
```
HTTP 404: Could not find the function public.exec_sql(query) in the schema cache
```

**Pourquoi √áa a √âchou√©**:
Supabase REST API ne fournit PAS de fonction `exec_sql` pour ex√©cuter du SQL arbitraire. La seule m√©thode est via le Dashboard SQL Editor ou les migrations.

**SOLUTION FINALE**: Cr√©ation Manuelle via Supabase Dashboard

1. Acc√®s au Supabase Dashboard
2. Navigation vers SQL Editor
3. Ex√©cution du SQL depuis `magicpath-project/supabase/migrations/create_articles_content.sql`
4. V√©rification avec:
```sql
SELECT * FROM articles_content LIMIT 1;
```

**R√©sultat**:
- ‚úÖ Table cr√©√©e avec succ√®s
- ‚úÖ Indexes cr√©√©s
- ‚úÖ Workflow peut maintenant sauvegarder les sections

---

### PROBL√àME 3: Workflow Bloqu√© (nextStep = draft_sections en boucle)

**Description**:
Apr√®s cr√©ation de la table, le test workflow retournait:
```json
{
  "nextStep": "draft_sections",
  "status": "pending"
}
```
...deux fois de suite, sans jamais ex√©cuter le step `draft_sections`.

**Sympt√¥mes**:
- Job restait en `currentStep: "draft_sections"` ind√©finiment
- Aucune section g√©n√©r√©e
- Temps de r√©ponse tr√®s rapide (24.6s) alors que 6 appels GPT-5.1 devraient prendre 3-5 minutes

**Diagnostic Initial (FAUX)**:
Suspicion que le code n'√©tait pas d√©ploy√© ou que le bloc `else if (step === 'draft_sections')` n'√©tait pas atteint.

**Investigation**:
1. V√©rification du code d√©ploy√© sur Vercel ‚úÖ
2. Confirmation de la pr√©sence du bloc draft_sections (lignes 1051-1197) ‚úÖ
3. V√©rification de la structure if/else if ‚úÖ

**D√âCOUVERTE R√âELLE**:
Le step `draft_sections` S'EX√âCUTAIT BIEN, mais plantait silencieusement!

**Preuve**:
```bash
curl -X POST .../api/geo -d '{"action":"workflow_step","jobId":"job_1763666425454_y59u3s"}'
# Temps d'ex√©cution: 38 secondes (pas 1 seconde)
# R√©ponse: {"error":"Unterminated string in JSON at position 9287"}
```

**Analyse du Job**:
```javascript
// job.logs contenait:
[
  { step: 'research', ... },
  { step: 'draft_section_0', ... },  // ‚úÖ Intro g√©n√©r√©e
  { step: 'draft_section_1', ... }   // ‚ùå Plantage ici
]

// articles_content DB:
// 1 section sauvegard√©e (Section 0: Intro, 1390 chars)
```

**Conclusion**:
Le workflow NE BOUCLAIT PAS. Il plantait lors de la g√©n√©ration de la Section 1 avec une erreur de parsing JSON.

---

## Probl√®me Actuel (BLOQUANT)

### PROBL√àME 4: Erreur de Parsing JSON - "Unterminated string"

**Description**:
GPT-5.1 retourne du JSON malform√© lors de la g√©n√©ration des sections.

**Erreur Exacte**:
```
Unterminated string in JSON at position 9287 (line 1 column 9288)
```

**√âtape d'√âchec**:
- Section 0 (Intro): ‚úÖ Succ√®s (1390 chars sauvegard√©s)
- Section 1 (Premier H2): ‚ùå √âchec lors du parsing JSON

**Cause Racine Probable**:

Le prompt demande √† GPT-5.1 de retourner du JSON structur√© comme:
```json
{
  "id": "section_1",
  "title": "Principes cl√©s du DevOps",
  "html": "<h2>Principes cl√©s du DevOps</h2><p>Le DevOps repose sur \"l'automatisation\"...</p>"
}
```

**Probl√®me**: Le contenu HTML contient:
1. Des guillemets non √©chapp√©s (`"l'automatisation"` au lieu de `\"l'automatisation\"`)
2. Des retours √† la ligne (`\n`) non √©chapp√©s
3. Potentiellement des caract√®res sp√©ciaux (√©mojis, accents)

**R√©sultat**: Le JSON devient invalide
```json
{
  "html": "<p>Le DevOps repose sur "l'automatisation" et...</p>"
          // ‚Üë Guillemets cassent la cha√Æne JSON
}
```

**Code Probl√©matique** (api/geo.ts, lignes 1063-1082 pour la Section 0):
```typescript
const introSys = 'You output ONLY compact JSON. Return strictly {"id":"intro","title":"Introduction","html":"..."} in French.';

const introUsr = `Tu es un expert GEO & SEO, sp√©cialiste Neil Patel.
R√©dige UNIQUEMENT le H1 et l'introduction d'un article (150-200 mots max).

SUJET: ${job.topic}
CONTEXTE: ${JSON.stringify(job.research || {}).slice(0, 3000)}

STRUCTURE:
- H1 titre SEO accrocheur (<h1>...</h1>)
- Introduction 150-200 mots:
  * Hook (stat ou question)
  * Promise (ce que le lecteur va apprendre)
  * Valeur (pourquoi c'est important)

Return JSON format: {"id":"intro","title":"Introduction","html":"<h1>...</h1><p>...</p>..."}`;

const introRes = await callAI('openai', 'gpt-5.1', [{role:'system', content: introSys}, {role:'user', content: introUsr}], 0.3, 2500);

// ‚ùå PARSING PEUT √âCHOUER ICI:
const introContent = stripFences((introRes?.content || '').trim());
const introData = JSON.parse(introContent); // üí• Unterminated string
```

**Fonction `callAI`** (lignes 1016-1020):
```typescript
const callAI = async (provider: string, model: string, messages: any, temperature = 0.3, maxTokens = 2000) => {
  const r = await fetch(`${base}/api/ai-proxy`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ provider, model, messages, temperature, maxTokens })
  });
  if (!r.ok) throw new Error(`${provider} ${model} ${r.status}`);
  return r.json();
};
```

**Fichier api-proxy** (probablement `/api/ai-proxy.ts`):
```typescript
// Appelle OpenAI API avec les param√®tres fournis
// MANQUE: response_format: { type: "json_object" }
```

**Pourquoi Ce Probl√®me N'Existait Pas Avant**:
Avec l'ancienne g√©n√©ration en 2 parties (8000 tokens), le HTML √©tait plus long et plus complexe, mais le prompt demandait du texte brut, pas du JSON structur√©.

---

## Solutions Propos√©es

### Option 1: Forcer JSON Mode OpenAI (RECOMMAND√â)

**Principe**: Utiliser le param√®tre `response_format` d'OpenAI pour garantir du JSON valide.

**Impl√©mentation**:

1. **Modifier la fonction `callAI`** (api/geo.ts, ligne 1016):
```typescript
const callAI = async (
  provider: string,
  model: string,
  messages: any,
  temperature = 0.3,
  maxTokens = 2000,
  responseFormat?: 'json_object' | 'text'  // Nouveau param√®tre
) => {
  const body: any = { provider, model, messages, temperature, maxTokens };

  // Ajouter response_format si demand√©
  if (responseFormat === 'json_object' && provider === 'openai') {
    body.response_format = { type: 'json_object' };
  }

  const r = await fetch(`${base}/api/ai-proxy`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(body)
  });

  if (!r.ok) throw new Error(`${provider} ${model} ${r.status}`);
  return r.json();
};
```

2. **Mettre √† jour tous les appels dans `draft_sections`**:
```typescript
// Ligne 1084 (Section 0 - Intro):
const introRes = await callAI('openai', 'gpt-5.1', [...], 0.3, 2500, 'json_object');

// Ligne 1118 (Sections 1-4 - H2):
const sectionRes = await callAI('openai', 'gpt-5.1', [...], 0.3, 2500, 'json_object');

// Ligne 1182 (Section 5 - FAQ):
const finalRes = await callAI('openai', 'gpt-5.1', [...], 0.3, 2500, 'json_object');
```

3. **Modifier `/api/ai-proxy.ts`** pour supporter `response_format`:
```typescript
// Dans la requ√™te OpenAI:
const openaiBody: any = {
  model: req.body.model,
  messages: req.body.messages,
  temperature: req.body.temperature,
  max_completion_tokens: req.body.maxTokens
};

// Ajouter response_format si fourni
if (req.body.response_format) {
  openaiBody.response_format = req.body.response_format;
}

const response = await fetch('https://api.openai.com/v1/chat/completions', {
  method: 'POST',
  headers: { /* ... */ },
  body: JSON.stringify(openaiBody)
});
```

**Avantages**:
- ‚úÖ OpenAI garantit du JSON valide
- ‚úÖ Pas de modification de prompts
- ‚úÖ Solution propre et maintenable

**Inconv√©nients**:
- N√©cessite modifications dans 2 fichiers (`geo.ts` + `ai-proxy.ts`)
- Support uniquement pour OpenAI (pas Perplexity)

---

### Option 2: Am√©liorer le Prompt (COMPL√âMENT)

**Principe**: Demander explicitement √† GPT-5.1 d'√©chapper correctement les caract√®res.

**Impl√©mentation**:

Modifier tous les prompts system dans `draft_sections`:
```typescript
// AVANT:
const introSys = 'You output ONLY compact JSON. Return strictly {"id":"intro","title":"Introduction","html":"..."} in French.';

// APR√àS:
const introSys = `You output ONLY valid JSON.
Rules:
- Escape all quotes in HTML: use \\" not "
- Escape all newlines: use \\n
- No line breaks in JSON
- Return format: {"id":"intro","title":"Introduction","html":"<h1>...</h1>"}

Example: {"html":"<p>L\\"automatisation est cl√©.</p>"}`;
```

**Avantages**:
- ‚úÖ Fonctionne avec tous les LLMs
- ‚úÖ Pas de modification d'API

**Inconv√©nients**:
- ‚ùå Pas de garantie √† 100% (LLM peut ignorer)
- ‚ùå Augmente la taille des prompts

---

### Option 3: Parser Plus Robuste (WORKAROUND)

**Principe**: R√©parer le JSON avant parsing.

**Impl√©mentation**:
```typescript
function safeParseJSON(text: string): any {
  let content = stripFences(text.trim());

  // Essayer parsing direct
  try {
    return JSON.parse(content);
  } catch (e1) {
    // Tentative 1: √âchapper les guillemets non √©chapp√©s dans le champ html
    try {
      const match = content.match(/"html"\s*:\s*"(.+)"/s);
      if (match) {
        const htmlContent = match[1]
          .replace(/\\"/g, '###QUOTE###')  // Prot√©ger les √©chapp√©s
          .replace(/"/g, '\\"')             // √âchapper les non √©chapp√©s
          .replace(/###QUOTE###/g, '\\"');  // Restaurer

        content = content.replace(match[0], `"html":"${htmlContent}"`);
        return JSON.parse(content);
      }
    } catch (e2) {}

    // Tentative 2: Extraire avec regex
    try {
      const idMatch = content.match(/"id"\s*:\s*"([^"]+)"/);
      const titleMatch = content.match(/"title"\s*:\s*"([^"]+)"/);
      const htmlMatch = content.match(/"html"\s*:\s*"(.+)"(?:\s*})?$/s);

      if (htmlMatch) {
        return {
          id: idMatch?.[1] || '',
          title: titleMatch?.[1] || '',
          html: htmlMatch[1]
        };
      }
    } catch (e3) {}

    throw new Error('Failed to parse JSON: ' + e1.message);
  }
}

// Usage:
const introData = safeParseJSON(introContent);
```

**Avantages**:
- ‚úÖ Fonctionne imm√©diatement
- ‚úÖ Pas de changement de prompts

**Inconv√©nients**:
- ‚ùå Code fragile et complexe
- ‚ùå Ne r√©sout pas la cause racine
- ‚ùå Difficile √† maintenir

---

### Option 4: D√©limiteurs au Lieu de JSON (ALTERNATIVE)

**Principe**: Abandonner JSON, utiliser des d√©limiteurs textuels.

**Impl√©mentation**:
```typescript
const introSys = `You output structured text with delimiters.
Format:
###ID###
intro
###TITLE###
Introduction
###HTML###
<h1>...</h1><p>...</p>
###END###`;

// Parsing:
const parts = introContent.split('###');
const id = parts[1]?.replace('ID', '').trim();
const title = parts[3]?.replace('TITLE', '').trim();
const html = parts[5]?.replace('HTML', '').trim();
```

**Avantages**:
- ‚úÖ Pas de probl√®me d'√©chappement
- ‚úÖ Plus simple pour le LLM

**Inconv√©nients**:
- ‚ùå Refonte compl√®te des prompts
- ‚ùå Moins structur√©
- ‚ùå Risque de collision avec contenu HTML

---

## Recommandation Finale

**SOLUTION HYBRIDE** (Option 1 + Option 2):

1. **Court terme (Fix Imm√©diat)**: Impl√©menter Option 1 (JSON Mode OpenAI)
2. **Moyen terme**: Ajouter Option 2 (Prompt am√©lior√©) comme s√©curit√© suppl√©mentaire
3. **Long terme**: Monitoring et alerting sur les erreurs de parsing

**Priorit√© de D√©ploiement**:
1. Modifier `/api/ai-proxy.ts` pour supporter `response_format`
2. Mettre √† jour `callAI` dans `api/geo.ts`
3. Ajouter `'json_object'` aux 3 appels dans `draft_sections`
4. Am√©liorer les prompts system pour √™tre plus explicites
5. Tester avec `node test-sectional-workflow.js`
6. Commit + Push pour d√©ploiement Vercel

---

## Fichiers Modifi√©s

### 1. `/api/geo.ts` (Fichier Principal)

**Lignes Cl√©s**:
- 45-89: Helper functions (saveSection, getAllSections, deleteSections)
- 1016-1020: callAI function (√Ä MODIFIER)
- 1051-1197: draft_sections step (6 sections √ó 2500 tokens)
- 1199-1262: assemble_article step

**Modifications Requises**:
```typescript
// Ligne 1016 - Ajouter param√®tre responseFormat
const callAI = async (
  provider: string,
  model: string,
  messages: any,
  temperature = 0.3,
  maxTokens = 2000,
  responseFormat?: 'json_object' | 'text'
) => { /* ... */ };

// Lignes 1084, 1118, 1182 - Ajouter 'json_object'
const introRes = await callAI('openai', 'gpt-5.1', [...], 0.3, 2500, 'json_object');
```

### 2. `/api/ai-proxy.ts`

**Modifications Requises**:
```typescript
// Ajouter support pour response_format
if (req.body.response_format && provider === 'openai') {
  openaiBody.response_format = req.body.response_format;
}
```

### 3. Supabase Migrations

**Fichier**: `magicpath-project/supabase/migrations/create_articles_content.sql`

**Status**: ‚úÖ D√©j√† ex√©cut√© manuellement

### 4. Scripts de Test Cr√©√©s

- `test-sectional-workflow.js`: Test complet du workflow
- `count-article-words.js`: Comptage de mots
- `analyze-job.js`: Analyse d√©taill√©e d'un job
- `check-storage-files.js`: V√©rification fichiers Supabase
- `download-from-supabase.js`: T√©l√©chargement d'articles
- `create-table-articles-content.js`: Tentative cr√©ation table (√©chou√©)

---

## Tests Effectu√©s

### Test 1: Workflow Sans Table (√âCHEC)
```bash
node test-sectional-workflow.js
# R√©sultat: Failed to save section 0: Could not find the table 'public.articles_content'
```

### Test 2: Cr√©ation Table Automatique (√âCHEC)
```bash
node create-table-articles-content.js
# R√©sultat: HTTP 404 - exec_sql function not found
```

### Test 3: Workflow Apr√®s Cr√©ation Manuelle (√âCHEC PARTIEL)
```bash
node test-sectional-workflow.js
# Job: job_1763666425454_y59u3s
# R√©sultat:
# - Section 0 (Intro): ‚úÖ Sauvegard√©e (1390 chars)
# - Section 1: ‚ùå Erreur JSON parsing (position 9287)
# - Status: error
```

### Test 4: Analyse D√©taill√©e
```bash
node analyze-job.js job_1763666425454_y59u3s

# R√©sultat:
# Status: error
# Current step: draft_sections
# Error: Unterminated string in JSON at position 9287
# Logs: 3 entr√©es (research, draft_section_0, draft_section_1)
# DB: 1 section sauvegard√©e
```

### Test 5: Appel Direct API
```bash
curl -X POST https://com-mmadimohamed.vercel.app/api/geo \
  -H "Content-Type: application/json" \
  -d '{"action":"workflow_step","jobId":"job_1763666425454_y59u3s"}'

# Temps: 38 secondes
# R√©sultat: {"error":"Unterminated string in JSON at position 9287 (line 1 column 9288)"}
```

---

## √âtat Actuel

### Ce Qui Fonctionne ‚úÖ

1. Architecture de workflow asynchrone
2. √âtape `research` (Perplexity Sonar)
3. Stockage en base de donn√©es (table `articles_content`)
4. G√©n√©ration de Section 0 (Intro)
5. Helpers DB (saveSection, getAllSections)
6. D√©ploiement Vercel automatique
7. Scripts de test et diagnostic

### Ce Qui Ne Fonctionne Pas ‚ùå

1. **G√©n√©ration des sections 1-5**: Erreur de parsing JSON
2. **Workflow complet**: Bloqu√© √† `draft_sections`
3. **Livraison d'articles**: Aucun article complet g√©n√©r√©

### Prochaines √âtapes

1. Impl√©menter JSON Mode OpenAI (Option 1)
2. Am√©liorer les prompts (Option 2)
3. Tester avec `job_1763666425454_y59u3s` (relancer workflow)
4. V√©rifier g√©n√©ration compl√®te des 6 sections
5. Tester `assemble_article`
6. Valider comptage de mots (> 2000)
7. Monitoring des erreurs en production

---

## Informations Techniques

### Environnement

- **URL Prod**: https://com-mmadimohamed.vercel.app
- **Supabase URL**: (voir .env.local)
- **OpenAI Model**: gpt-5.1-turbo-2025-07-15
- **Perplexity Model**: sonar
- **Node Version**: (√† v√©rifier)
- **Vercel R√©gion**: (√† v√©rifier)

### Limites Connues

- Timeout Vercel Edge Functions: 25 secondes (peut poser probl√®me pour 6 √ó GPT-5.1)
- Max tokens GPT-5.1: 2500 par section (suffisant pour ~500-700 mots)
- Supabase Free Tier: 500 MB storage, 2 GB bandwidth/mois

### M√©triques Cibles

- Article complet: 2000-3500 mots
- Temps g√©n√©ration total: 3-5 minutes (6 sections)
- Co√ªt GPT-5.1: ~$0.05-0.10 par article (estimation)
- Taux succ√®s attendu: > 95%

---

## Contact & Support

Pour toute question ou suggestion sur ce rapport, contacter l'√©quipe de d√©veloppement.

**Derni√®re mise √† jour**: 2025-11-20 19:30 UTC
